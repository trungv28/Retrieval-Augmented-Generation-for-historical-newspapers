{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import cohere\n",
    "import numpy as np\n",
    "# from rank_bm25 import BM25Okapi\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096d49c-d3dc-4329-ada7-aff56d210198",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = 'llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c63e1-4c2f-439d-8d95-4c6aa01f41cf",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_name = \"gpl\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "vectordb = Chroma(persist_directory=\"corpus_db\", embedding_function = hf)\n",
    "titledb = Chroma(persist_directory=\"title_db\", embedding_function=hf)\n",
    "top_retrieve = 20\n",
    "\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\",\n",
    "        search_kwargs={'k': top_retrieve, 'lambda_mult': 0.25}\n",
    ")\n",
    "title_retriever = titledb.as_retriever()\n",
    "compressor = CohereRerank(model = 'rerank-multilingual-v3.0', top_n = top_retrieve)\n",
    "web_search_tool = TavilySearchResults(k = 3)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tagger = SequenceTagger.load(\"hmbert/flair-hipe-2022-newseye-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(docs, question):\n",
    "    rerank_docs = compressor.compress_documents(docs, question)\n",
    "    texts = [doc.page_content for doc in rerank_docs]\n",
    "    ners = [doc.metadata['ner'] for doc in rerank_docs]\n",
    "    sentence = Sentence(question)\n",
    "    tagger.predict(sentence)\n",
    "    sen_dict = sentence.to_dict(tag_type='ner')\n",
    "    aner = \" \".join([ner['labels'][0]['value'] for ner in sen_dict['entities']] + ['O'])\n",
    "\n",
    "    all_ner = ners + [aner]\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_ner)\n",
    "    query_vector = tfidf_matrix[-1]\n",
    "    doc_vectors = tfidf_matrix[:-1]\n",
    "    ner_scores = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    co_scores = np.array([float(doc.metadata['relevance_score']) for doc in rerank_docs])\n",
    "\n",
    "    scores = 0.8 * co_scores + 0.2 * ner_scores\n",
    "    max_idx = np.argsort(-scores)\n",
    "    final_docs = []\n",
    "    for idx in max_idx[:3]:\n",
    "        if scores[idx] > 0.5:\n",
    "            final_docs.append(texts[idx])\n",
    "    return final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d531a81-6d4d-405e-975a-01ef1c9679fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0.3)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ff2db-eb4e-4d44-904c-ea061abc16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_tool = TavilySearchResults(k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa3d08-6a86-4705-a28b-e2721070bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents \n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str \n",
    "    web_search : str\n",
    "    title: List[str]\n",
    "    documents : List[str]\n",
    "\n",
    "\n",
    "def title_retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve titles from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    # print(\"---TITLE RETRIEVE---\")\n",
    "    question = state['question']\n",
    "\n",
    "    titles = title_retriever.invoke(question)\n",
    "    title = [t.page_content for t in titles]\n",
    "    return {'title': title, 'question': question}\n",
    "    # return {'question': question}\n",
    "    \n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    title = state['title']\n",
    "    docs = retriever.invoke(question)\n",
    "    refined_docs = rerank(docs, question)\n",
    "    return {\"documents\": refined_docs, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    \n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    title = state['title']\n",
    "    # print(title)\n",
    "    # if title:\n",
    "    #     print(\"---ROUTE QUESTION TO RAG---\")\n",
    "    #     return \"vectorstore\"\n",
    "    # else:\n",
    "    #     print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "    #     return \"websearch\"\n",
    "    print(\"---ROUTE QUESTION TO RAG---\")\n",
    "    return 'vectorstore'\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Check if any documents are related\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    \n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    docs = state['documents']\n",
    "    if docs: return 'generate'\n",
    "    return 'websearch'\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"title_retrieve\", title_retrieve)\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4b9e4-3ba8-47d6-958c-e5a7112ac6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"title_retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"title_retrieve\",\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13043b0f-17c7-49d3-9ea7-8f2c0f0c8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()\n",
    "\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Qui est Caros Sadoval\"}\n",
    "print(inputs)\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
